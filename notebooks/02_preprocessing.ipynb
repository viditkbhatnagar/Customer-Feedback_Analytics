{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viditkbhatnagar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config/config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextPreprocessor\n\u001b[0;32m---> 31\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mTextPreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 3. Load Raw Data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/customer_reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/codes/customer_feedback_analytics/notebooks/../src/data_processing/preprocessor.py:37\u001b[0m, in \u001b[0;36mTextPreprocessor.__init__\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize preprocessor with configuration\"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config/config.yaml'"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing - Customer Feedback Analytics\n",
    "# This notebook demonstrates the text preprocessing pipeline for customer reviews.\n",
    "\n",
    "# 1. Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "print(\"Libraries loaded successfully!\")\n",
    "\n",
    "# 2. Import custom preprocessor\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data_processing.preprocessor import TextPreprocessor\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# 3. Load Raw Data\n",
    "df = pd.read_csv('../data/raw/customer_reviews.csv')\n",
    "print(f\"Loaded {len(df)} reviews\")\n",
    "display(df.head())\n",
    "\n",
    "# 4. Text Cleaning Steps (demonstration on one sample)\n",
    "sample_text = df['review_text'].iloc[0]\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 1: Basic cleaning\n",
    "cleaned_text = preprocessor.clean_text(sample_text)\n",
    "print(\"After basic cleaning:\")\n",
    "print(cleaned_text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 2: Handle negations\n",
    "negation_handled = preprocessor.handle_negations(cleaned_text)\n",
    "print(\"After handling negations:\")\n",
    "print(negation_handled)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 3: Remove stopwords\n",
    "no_stopwords = preprocessor.remove_stopwords(negation_handled)\n",
    "print(\"After removing stopwords:\")\n",
    "print(no_stopwords)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 4: Lemmatization\n",
    "lemmatized = preprocessor.lemmatize_text(no_stopwords)\n",
    "print(\"After lemmatization:\")\n",
    "print(lemmatized)\n",
    "\n",
    "# 5. Apply Preprocessing to Dataset\n",
    "print(\"Applying preprocessing pipeline...\")\n",
    "df_processed = preprocessor.preprocess_dataset(df)\n",
    "print(f\"\\nPreprocessing complete!\\nOriginal shape: {df.shape}\\nProcessed shape: {df_processed.shape}\")\n",
    "print(f\"New columns added: {set(df_processed.columns) - set(df.columns)}\")\n",
    "\n",
    "# 6. Feature Analysis\n",
    "feature_cols = [col for col in df_processed.columns if col not in df.columns]\n",
    "print(f\"New features created: {len(feature_cols)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(\"- Text statistics:\", [col for col in feature_cols if 'count' in col or 'length' in col or 'ratio' in col])\n",
    "print(\"- Sentiment features:\", [col for col in feature_cols if 'textblob' in col or 'positive' in col or 'negative' in col])\n",
    "print(\"- Time features:\", [col for col in feature_cols if 'hour' in col or 'dayofweek' in col or 'month' in col])\n",
    "\n",
    "feature_stats = df_processed[feature_cols].describe()\n",
    "display(feature_stats.round(2))\n",
    "\n",
    "# 7. Text Transformation Analysis\n",
    "comparison_df = pd.DataFrame({\n",
    "    'original': df['review_text'].head(5),\n",
    "    'cleaned': df_processed['cleaned_text'].head(5)\n",
    "})\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\nReview {idx + 1}:\\nOriginal: {row['original'][:100]}...\\nCleaned: {row['cleaned'][:100]}...\\n\" + \"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df['original_word_count'] = df['review_text'].str.split().str.len()\n",
    "axes[0].hist(df['original_word_count'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Original Text Word Count Distribution')\n",
    "axes[0].axvline(df['original_word_count'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"original_word_count\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "axes[1].hist(df_processed['word_count'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Cleaned Text Word Count Distribution')\n",
    "axes[1].axvline(df_processed['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {df_processed[\"word_count\"].mean():.1f}')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Average reduction in word count: {(1 - df_processed['word_count'].mean() / df['original_word_count'].mean()) * 100:.1f}%\")\n",
    "\n",
    "# 8. Sentiment Feature Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(df_processed['textblob_polarity'], bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Polarity Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('TextBlob Polarity Distribution')\n",
    "axes[0].axvline(0, color='red', linestyle='--', label='Neutral')\n",
    "axes[0].legend()\n",
    "axes[1].hist(df_processed['textblob_subjectivity'], bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Subjectivity Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('TextBlob Subjectivity Distribution')\n",
    "axes[1].axvline(0.5, color='red', linestyle='--', label='Mid-point')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sentiment_features = ['textblob_polarity', 'positive_word_count', 'negative_word_count']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, feature in enumerate(sentiment_features):\n",
    "    df_processed.boxplot(column=feature, by='true_sentiment', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by True Sentiment')\n",
    "    axes[idx].set_xlabel('True Sentiment')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Special Character Analysis\n",
    "punct_features = ['exclamation_count', 'question_count', 'caps_ratio']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, feature in enumerate(punct_features):\n",
    "    sentiment_groups = df_processed.groupby('true_sentiment')[feature].mean().sort_values()\n",
    "    colors = {'positive': '#2ECC71', 'negative': '#E74C3C', 'neutral': '#95A5A6'}\n",
    "    bar_colors = [colors.get(x, 'gray') for x in sentiment_groups.index]\n",
    "    axes[idx].bar(sentiment_groups.index, sentiment_groups.values, color=bar_colors)\n",
    "    axes[idx].set_xlabel('Sentiment')\n",
    "    axes[idx].set_ylabel(f'Average {feature}')\n",
    "    axes[idx].set_title(f'{feature} by Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 10. Word Cloud Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "for idx, sentiment in enumerate(['positive', 'negative']):\n",
    "    text = ' '.join(df[df['true_sentiment'] == sentiment]['review_text'].dropna())\n",
    "    wordcloud = WordCloud(width=600, height=400, background_color='white',\n",
    "                         colormap='Greens' if sentiment == 'positive' else 'Reds').generate(text)\n",
    "    axes[0, idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[0, idx].set_title(f'Original - {sentiment.capitalize()} Reviews')\n",
    "    axes[0, idx].axis('off')\n",
    "for idx, sentiment in enumerate(['positive', 'negative']):\n",
    "    text = ' '.join(df_processed[df_processed['true_sentiment'] == sentiment]['cleaned_text'].dropna())\n",
    "    wordcloud = WordCloud(width=600, height=400, background_color='white',\n",
    "                         colormap='Greens' if sentiment == 'positive' else 'Reds').generate(text)\n",
    "    axes[1, idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[1, idx].set_title(f'Cleaned - {sentiment.capitalize()} Reviews')\n",
    "    axes[1, idx].axis('off')\n",
    "plt.suptitle('Word Clouds: Original vs Cleaned Text', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 11. Feature Correlation Analysis\n",
    "corr_features = ['rating', 'word_count', 'exclamation_count', 'question_count',\n",
    "                'caps_ratio', 'textblob_polarity', 'textblob_subjectivity',\n",
    "                'positive_word_count', 'negative_word_count']\n",
    "corr_matrix = df_processed[corr_features].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12. Save Preprocessed Data and Stats\n",
    "output_path = '../data/processed/preprocessed_reviews.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"Preprocessed data saved to: {output_path}\")\n",
    "\n",
    "preprocessing_stats = {\n",
    "    'original_reviews': len(df),\n",
    "    'processed_reviews': len(df_processed),\n",
    "    'reviews_filtered': len(df) - len(df_processed),\n",
    "    'avg_word_reduction': (1 - df_processed['word_count'].mean() / df['original_word_count'].mean()) * 100,\n",
    "    'features_added': len(feature_cols),\n",
    "    'avg_textblob_polarity': df_processed['textblob_polarity'].mean(),\n",
    "    'avg_textblob_subjectivity': df_processed['textblob_subjectivity'].mean()\n",
    "}\n",
    "import json, os\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "with open('../results/preprocessing_stats.json', 'w') as f:\n",
    "    json.dump(preprocessing_stats, f, indent=2)\n",
    "print(\"\\nPreprocessing statistics:\")\n",
    "for key, value in preprocessing_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
